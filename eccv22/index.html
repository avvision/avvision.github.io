<!doctype html>
<html>
<head>
    <meta charset="utf-8">

    <title>Homepage</title>
    <link rel="icon" href="favicon.png" type="image/png">

    <link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,800italic,700italic,600italic,400italic,300italic,800,700,600' rel='stylesheet' type='text/css'>

    <link href="css/bootstrap.css" rel="stylesheet" type="text/css">
    <link href="css/style.css" rel="stylesheet" type="text/css">
    <link href="css/font-awesome.css" rel="stylesheet" type="text/css">
    <link href="css/animate.css" rel="stylesheet" type="text/css">


    <script type="text/javascript" src="js/jquery.1.8.3.min.js"></script>
    <script type="text/javascript" src="js/bootstrap.js"></script>
    <script type="text/javascript" src="js/jquery-scrolltofixed.js"></script>
    <script type="text/javascript" src="js/jquery.easing.1.3.js"></script>
    <script type="text/javascript" src="js/jquery.isotope.js"></script>
    <script type="text/javascript" src="js/wow.js"></script>
    <script type="text/javascript" src="js/classie.js"></script>
    <script src="contactform/contactform.js"></script>


    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-173267821-2"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-173267821-2');
    </script>

    <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1,user-scalable=no" />
</head>

<style>
    /*自定义的H0 H1 H2类型*/
    h0.Program {
        color: black;
        font-size: 16px;
        text-align: left;
    }
    h1.Program {
        color: black;
        font-size: 27px;
        text-align: left;
    }
    h2.Program {
        color: black;
        font-size: 20px;
        text-align: left;
    }
</style>

<style>
    /*body{transform: scale(1);}*/

    span {
    color: #888888;
    font-family: 'Open Sans', sans-serif;
    /*font-size: 1em; */ /*原来是16px*/
    font-size: 1em;
    /* font-size: 16px; */
    font-weight: 450;
    }


    div.container {
    font-size: 1.6rem;
    text-align: justify;
    /* text-justify: inter-word; */
    }  

    div.nav {
    text-align: center;
    /* text-justify: inter-word; */
    }  

    div.pc {
    text-align: center;
    /* position: relative; */
    /* margin-bottom: -5rem; */
    /* text-justify: inter-word; */
    }  
    div.start {
    text-align: left;

    font-size: 0.8rem;
    /* position: relative; */
    margin-bottom: 15rem;
    /* text-justify: inter-word; */
    }
    div.img {
        float: right;
        width: 15rem;
        height: 15rem;
    }
</style>




<body>
<header class="header" id="header">
        
    <div class="start">
        <a href="https://avvision.xyz/"><div class="img"><img src="img/logo.png" class="right"></div></a>

        <h0 class="animated fadeInDown delay-07s" >3rd AVVision Workshop</h0>
        <ul class="we-create animated fadeInUp delay-1s">
            <br>
            <h0>October 23rd at ECCV 2022</h0>
        </ul>
    </div>

    <br>

</header>


<nav class="main-nav-outer" id="test"> <!--main-nav-start-->
    <div class="nav">
        <ul class="main-nav"> <!--可以改bar的字体大小-->
            <li><a href="#about" >About</a></li>
            <li><a href="#speakers" >Speakers</a></li>
            <li><a href="#organizers" >Organizers</a></li>
            <li><a href="#submission" >Submission</a></li>
            <li><a href="#Program">Program</a></li>
            <li><a href="#sponsor">Sponsors</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </div>
</nav>


<section class="main-section paddind" id="about">
    <div class="container">
        <h2>About</h2>

        The 3rd Autonomous Vehicle Vision (AVVision) Workshop is part of the Advanced Autonomous Driving Workshop (<a href="https://eccv2022.ecva.net/program/">AADW</a>), 
        organized in conjunction with ECCV 2022. 
        The other jointly organized workshop is the 2nd workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous Driving (<a href="https://ssladcompetition.github.io/ssladcompetition/index.html">SSLAD</a>).
        <br>
        <br>
        The 3rd AVVision workshop aims to bring together industry professionals and academics to brainstorm and exchange ideas on the advancement of computer vision techniques for autonomous driving.
        In this half-day workshop, we will have four keynote talks and regular paper presentations (oral and poster) to discuss the state-of-the-art and existing challenges in autonomous driving.
   
    </div>
</section>

<br>

<section class="main-section paddind" id="speakers">
    <div class="container " >
        <h2>Speakers</h2>

        <div class="portfolioContainer_new wow fadeInUp delay-02s text-align:center" >

            <div class="Portfolio-box branding">
                <a href="http://www.cvlibs.net/"><img src="img/andreas.png" alt=""></a>
                <h3>Andreas Geiger</h3>
                <p>Full professor at University of Tübingen</p>
            </div>

            <div class="Portfolio-box photography" >
                <a href="https://www.yf.io/"><img src="img/fisher.png" alt=""></a>
                <h3>Fisher Yu</h3>
                <p>Assistant Professor at ETH Zürich</p>
            </div>
            
            <div class="Portfolio-box printdesign" >
                <a href="http://www.cs.unc.edu/~wliu/"><img src="img/wei_liu.png" alt=""></a>
                <h3>Wei Liu</h3>
                <p>Head of Machine Learning Research at Nuro</p>
            </div>

            <div class="Portfolio-box webdesign">
                <a href="http://www.cs.toronto.edu/~urtasun/"><img src="img/raquel.png" alt=""></a>
                <h3>Raquel Urtasun</h3>
                <p>Founder and CEO of Waabi, Full Professor at University of Toronto</p>
            </div>
            
        </div>
    </div>


    <div class="container" style="margin-top: 1rem">
        <div class="portfolioContainer_new wow fadeInUp delay-02s" id="my_speakers">


        </div>

    </div>
</section>

<br>

<section class="main-section paddind" id="organizers">
    <div class="container">
        <h2>Organizers</h2>

        <div class="portfolioContainer_new wow fadeInUp delay-02s text-align:center" >

            <div class="Portfolio-box printdesign" >
                <a href="https://www.ruirangerfan.com/"><img src="img/rui_fan.png" alt=""></a>
                <h3>Rui Ranger Fan</h3>
                <p>Tongji University</p>
            </div>

            <div class="Portfolio-box webdesign">
                <a href="https://djurikom.github.io/"><img src="img/nemanja.png" alt=""></a>
                <h3>Nemanja Djuric</h3>
                <p>Aurora Innovation</p>
            </div>

            <div class="Portfolio-box branding">
                <a href="https://wenshuowang.github.io/"><img src="img/wenshuo_wang.png" alt=""></a>
                <h3>Wenshuo Wang</h3>
                <p>McGill University</p>
            </div>

            <div class="Portfolio-box photography" >
                <a href="https://www.ondruska.com/"><img src="img/PeterOndruska.jpg" alt=""></a>
                <h3>Peter Ondruska</h3>
                <p>Toyota WP</p>
            </div>

            <div class="Portfolio-box photography" >
                <a href="https://www.tri.global/about-us/jie-li/"><img src="img/JieLi.png" alt=""></a>
                <h3>Jie Li</h3>
                <p>Toyota Research Institute</p>
            </div>

        </div>
    </div>


        <br>

        <div class="pc">
            <h6>Program Committee</h6>
                
                <div class="pc-member">
                    <h4 style="float: left">
                        Qijun Chen 
                        <span>
                        Tongji University
                        </span>
                        Ming Liu
                        <span>
                        HKUST
                        </span>
                        Junhao Xiao
                        <span>
                        NUDT
                        </span>
                        Yanjun Huang
                        <span>
                        Tongji University
                        </span>
                        Fei Gao
                        <span>
                        Zhejiang University
                        </span>
                        Shuai Su 
                        <span>
                        Tongji University
                        </span>
                        Hengli Wang
                        <span>
                        HKUST
                        </span>
                        Jiayuan Du
                        <span>
                        Tongji University
                        </span>
                        Jianhao Jiao
                        <span>
                        HKUST
                        </span>
                        Peng Yun
                        <span>
                        HKUST
                        </span>
                        Hesham Eraqi
                        <span>
                        AUC
                        </span>
                        Xinshuo	Weng
                        <span>
                        NVIDIA
                        </span>
                        Joshua	 Manela
                        <span>
                        Waymo
                        </span>
                        Vladan	Radosavljevic
                        <span>
                        Spotify
                        </span>
                        Abhishek Mohta
                        <span>
                        Aurora Innovation
                        </span>
                        Sebastian Lopez-Cot
                        <span>
                        Aurora Innovation
                        </span>
                        Yan Xu
                        <span>
                        Carnegie Mellon University
                        </span>
                    </h4>
                </div>

                <div class="pc-member">
                    <h4 style="float: left">
                        Ming Yang
                        <span>
                        SJTU
                        </span>
                        Xiang Gao
                        <span>
                        Idriverplus
                        </span>
                        Chengju Liu
                        <span>
                        Tongji University
                        </span>
                        M. Junaid Bocus
                        <span>
                        University of Bristol
                        </span>
                        Yi Zhou
                        <span>
                        Hunan University
                        </span>
                        Hong Wei
                        <span>
                        Reading University
                        </span>
                        Chaoqun Wang
                        <span>
                        Shandong University
                        </span>
                        Yi Feng
                        <span>
                        Tongji University
                        </span>
                        Deming Wang
                        <span>
                        Tongji University
                        </span>
                        Mengjiao Shen
                        <span>
                        Tongji University
                        </span>
                        Bohuan Xue
                        <span>
                        HKUST
                        </span>
                        Dequan Wang
                        <span>
                        University of California, Berkeley
                        </span>
                        Peide Cai
                        <span>
                        HKUST
                        </span>
                        Nachuan Ma
                        <span>
                        Tongji University
                        </span>
                        Huaiyang Huang
                        <span>
                        HKUST
                        </span>
                        Meng Fan
                        <span>
                        Aurora Innovation
                        </span>
                    </h4>
                </div>

                <div class="pc-member">
                    <h4 style="float: left">
                        Yong Liu
                        <span>
                        Zhejiang University
                        </span>
                        Xiaolin Huang
                        <span>
                        SJTU
                        </span>
                        Lei Qiao
                        <span>
                        SJTU
                        </span>
                        Wei Ye
                        <span>
                        Tongji University
                        </span>
                        Sen Jia
                        <span>
                        Toronto AI Lab, LG Electronics
                        </span>
                        Sicen Guo
                        <span>
                        Tongji University
                        </span>
                        Jingwei Yang
                        <span>
                        Tongji University
                        </span>
                        Jiahe Fan
                        <span>
                        Tongji University
                        </span>
                        Zhuwen Li
                        <span>
                        Nuro, Inc.
                        </span>
                        Zhaoen Su
                        <span>
                        Aurora Inovation
                        </span>
                        Yixin Fei 
                        <span>
                        Tongji University
                        </span>
                        Shivam Gautam
                        <span>
                        Aurora Inovation
                        </span>
                        Henggang Cui
                        <span>
                        Motional
                        </span>
                        Fang-Chieh Chou
                        <span>
                        Aurora Inovation
                        </span>
                        Tanmay Agarwal
                        <span>
                        Argo AI
                        </span>
                        Shreyash Pandey
                        <span>
                        Aurora Innovation
                        </span>
                        <span>
                        </span>
                        <span>
                        </span>
                    </h4>
                </div>
        </div>
    </div>
</section>

<br>

<section class="main-section paddind" id="submission">
    <div class="container">
        <h2>Submission</h2>
        <h6 style="margin-bottom: 15px">Call for Papers</h6>
        With a number of breakthroughs in autonomous system technology over the past decade, the race to commercialize self-driving
        cars has become fiercer than ever. The integration of advanced sensing, computer vision, signal/image processing, and machine/deep learning into autonomous
        vehicles enables them to perceive the environment intelligently and navigate safely. Autonomous driving is required to ensure safe, reliable, and efficient automated
        mobility in complex uncontrolled real-world environments. Various applications range from automated transportation and farming to public safety and environment exploration.
        Visual perception is a critical component of autonomous driving. Enabling technologies include: a) affordable sensors that can acquire useful data under
        varying environmental conditions, b) reliable simultaneous localization and mapping, c) machine learning that can effectively handle varying real-world conditions and unforeseen
        events, as well as “machine-learning friendly” signal processing to enable more effective classification and decision making, d) hardware and software co-design for efficient real-time
        performance, e) resilient and robust platforms that can withstand adversarial attacks and failures, and f) end-to-end system integration of sensing, computer vision, signal/image
        processing and machine/deep learning. The 3rd AVVision workshop will cover all these topics. Research papers are solicited in, but not limited to, the following topics:
        <br>
        <ul>

            <li> 3D road/environment reconstruction and understanding</li>
            <li> Mapping and localization for autonomous cars</li>
            <li> Semantic/instance driving scene segmentation and semantic mapping</li>
            <li> Self-supervised/unsupervised visual environment perception</li>
            <li> Car/pedestrian/object/obstacle detection/tracking and 3D localization</li>
            <li> Car/license plate/road sign detection and recognition</li>
            <li> Driver status monitoring and human-car interfaces</li>
            <li> Deep/machine learning and image analysis for car perception</li>
            <li> Adversarial domain adaptation for autonomous driving</li>
            <li> On-board embedded visual perception systems</li>
            <li> Bio-inspired vision sensing for car perception</li>
            <li> Real-time deep learning inference</li>
        </ul>

        <br>
        <h6 style="margin-bottom: 15px; margin-top: 20px" >Important Dates</h6>
        <ul>
            <li> Submission deadline: <s>July 31th, 2022 (anywhere on Earth)</s> </li>
            <li> Review feedback release date: <s>August 16th, 2022</s> </li>
            <li> Camera-ready submission: <s>August 22nd, 2022</s> </li>
            <li> Workshop date: <s>October 23rd, 2022</s></li>
        </ul>

        <br>
        <h6 style="margin-bottom: 15px; margin-top: 20px" >Submission Guidelines</h6>
        <b>Regular papers</b>: Authors are encouraged to submit high-quality, original
        (i.e., not been previously published or accepted for publication
        in substantially similar form in any peer-reviewed venue 
        including journal, conference or workshop) research.

        The paper template is identical to the <a href="https://eccv2022.ecva.net/submission/call-for-papers/">ECCV 2022</a> main conference.
        Papers are limited to 14 pages, including figures and tables, 
        in the ECCV style. Additional pages containing only cited 
        references are allowed. Please refer to the following files for 
        detailed formatting instructions:

        <br>
        <ul>
        <li>Example submission paper with detailed instructions [<a href="https://drive.google.com/file/d/170gt4Em4cb1yh_Vq2V-i1DUeYIyJCcDq/view?usp=sharing">download</a>]</li>
        <li>LaTeX Templates: eccv2022AuthorKit.zip [<a href="https://drive.google.com/file/d/171-xo72Jx40cZ4qT20ZZsDDKXPIOWnss/view?usp=sharing">download</a>]</li>
        </ul>
    
        Papers that are not properly anonymized, or do not use the template, or have more than fourteen pages (not excluding references) will be rejected without review. 
        The <a href="https://easychair.org/conferences/?conf=avvision2022">submission site</a> is now open. 

        <br>
        <br>
        <b>Extended abstracts:</b> We encourage participants to submit preliminary ideas that have not been published before as 
        extended abstracts. These submissions would benefit from additional exposure and discussion that can shape a better 
        future publication. We also invite papers that have been published at other venues to spark discussions and foster new collaborations. 
        Submissions may consist of up to 7 pages plus one additional page solely for references (using the template detailed above). The extended abstracts will <b>NOT</b> be published in the workshop proceedings.





    </div>

</section>


<!-- <section class="main-section paddind" id="Program">
    <div class="container ">
        <h2>Program (Eastern Time)</h2>

        <h1>
        <b>Opening Remark</b>: 07:00--07:05
        </h1>
        <h1>
        <b>Keynote Session I</b>: 07:05--08:25
        </h1>

        07:05--07:45 Andreas Geiger<br>
        07:45--08:25 Fisher Yu<br>
        <h1>
        <b>Oral Paper Session I</b>: 08:25--09:25 <br>
        </h1>

        <h1>
        <b>Keynote Session II</b>: 09:25--10:45
        </h1>
        09:25--10:05 Raquel Urtasun<br>
        10:05--10:45 Matthew Johnson-Roberson<br>
        <h1>

            
        <h1>
        <b>Poster Paper Session</b>: 10:45--13:25
        </h1>

        <h1>
        <b>Lunch Break</b>:  13:25--13:55 
        </h1>

        <h1>
        <b>Keynote Session III</b>: 13:55--15:55 
        </h1>
        13:55--14:35 Laura Leal-Taixé <br>
        14:35--15:15 Cordelia Schmid  <br>
        15:15--15:55 Carl Wellington <br>

        <h1><b>Oral Paper Session II</b>: 15:55--16:55 <br>
        </h1>

        <h1>
        <b>Closing Remark</b>: 16:55--17:00 
        </h1>
        
    </div>
</section> -->

<section class="main-section paddind" id="Program">
    <div class="container ">
        <h2>Program (Local Tel Aviv Time)</h2>

        <h1>
        <b>Opening Remarks</b>: 14:00-14:05
        </h1>

        <h1>
        <b>Invited Talk I</b>: 14:05-14:40
        </h1>
        <!-- <h8></h8> -->

        <ul>
        <li>Raquel Urtasun (Founder and CEO of Waabi, Full Professor at University of Toronto)</li>
        </ul>

        <h1>
        <b>Invited Talk II</b>: 14:40-15:15
        </h1>

        <ul>
        <li>Andreas Geiger (Full professor at University of Tübingen)</li>
        </ul>

       <b>Title:</b>  Learning Robust Policies for Self-Driving
        <br>
        <b>Abstract:</b> In this talk I will present 3 recent results from my lab: First, I will present TransFuser (PAMI 2022) which tackles the question on how representations from complementary sensors should be integrated for autonomous driving. Second, I will discuss PlanT (CoRL 2022), our recent efforts towards explainable planning for autonomous driving. Finally, I will present KING (ECCV 2022), an approach to generate novel safety-critical driving scenarios for improving the robustness of imitation learning based self-driving agents.

        <h1>
        <b>Lightning Talks I</b>: 15:15-15:55 <br>
        </h1>

        <ol>
        <li><em>"4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation"</em>, Lars Kreuzberg, Idil Esen Zulfikar, Sabarinath Mahadevan, Francis Engelmann and Bastian Leibe (<a href="https://drive.google.com/file/d/1CgNeCVXX_Mat9-B-NN-wfQF5pvp5wC7J/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=DJXJTnhdZfg">video</a>)</li>
        <li><em>"BlindSpotNet: Seeing Where We Cannot See"</em>, Taichi Fukuda, Kotaro Hasegawa, Shinya Ishizaki, Shohei Nobuhara and Ko Nishino (<a href="https://drive.google.com/file/d/18qA1KIFudpdXfxA_kPFMLkCCnp3B6z5x/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=8bMcoIZ9teI">video</a>)</li>
        <li><em>"Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles"</em>, Adrian Holzbock, Nicolai Kern, Christian Waldschmidt, Klaus Dietmayer and Vasileios Belagiannis (<a href="https://drive.google.com/file/d/1acYhccK2Tk1ApznCPYYkEofOZSx-blwK/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=RnqLWfsXHjQ">video</a>)</li>
        <li><em>"An improved lightweight network based on YOLOv5s for object detection in autonomous driving"</em>, Guofa Li, Yingjie Zhang, Delin Ouyang and Xingda Qu (<a href="https://drive.google.com/file/d/1B7Pev_9fiBFkK_q9_qHC0DurdqkeC0X0/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=iq8ZYIqnnyI">video</a>)</li>
        <li><em>"Plausibility Verification For 3D Object Detectors Using Energy-Based Optimization"</em>, Abhishek Vivekanandan, Niels Maier and J. Marius Zöllner (<a href="https://drive.google.com/file/d/1BlSSQqnExVTorfArK6NI4cTlBH796FFa/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=grU2WbWL6h8">video</a>)</li>
        <li><em>"Lane Change Classification and Prediction with Action Recognition Networks"</em>, Kai Liang, Jun Wang and Abhir Bhalerao (<a href="https://drive.google.com/file/d/1l654Wlke1TJsD9A0EwUCNUz4oHLH2GRj/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=jdkVk2s9ssA">video</a>)</li>
        <li><em>"Joint Prediction of Amodal and Visible Semantic Segmentation for Automated Driving"</em>, Jasmin Breitenstein, Jonas Löhdefink and Tim Fingscheidt (<a href="https://drive.google.com/file/d/1Vmob3fQo9VXk2DbtWuH4WjOAdUD4olN4/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=g7UOfb60Qjo">video</a>)</li>
        <li><em>"Leveraging Geometric Structure for Label-efficient Semi-supervised Scene Segmentation"</em>, Ping Hu, Stan Sclaroff and Kate Saenko (<a href="https://drive.google.com/file/d/1Ui-zeJ7O5HDGoEWbSZ4lXcfHf2XFc8z8/view?usp=sharing">extended abstract</a>, <a href="https://www.youtube.com/watch?v=HnU_q7L-lmg">video</a>)</li>
        </ol>

        <h1>
        <b>Virtual Coffee Break</b>: 15:55-16:05 <br>
        </h1>

        <h1>
        <b>Invited Talk III</b>: 16:05-16:40
        </h1>

        <ul>
        <li>Fisher Yu (Assistant Professor at ETH Zürich)</li>
        </ul>

       <b>Title:</b>  Towards High-Quality 4D Scene Understanding in Autonomous Driving
       <br>
        <b>Abstract:</b> Understanding semantics and motion in dynamic 3D scenes is foundational for autonomous driving. The recent availability of large-scale driving video datasets creates new research possibilities in this broad area. In this talk, I will illustrate the trend through the lens of object tracking, the essential building block for dynamic scene understanding. I will start with our recent findings in multiple object tracking (MOT), after briefly reviewing the current works and trends on the topic. Then, I will introduce our new tracking method based on Quasi-Dense Similarity Learning. Our method is conceptually more straightforward yet more effective than the previous works. It boasts almost ten percent of accuracy on the Waymo MOT dataset. I will also talk about how to use the 2D tracking method for monocular 3D object tracking and video instance segmentation. Our quasi-dense 3D tracking pipeline achieves impressive improvements on the nuScenes 3D tracking benchmark and achieve the state of the art by utilizing panoramic views.
        
        <h1>
        <b>Invited Talk IV</b>: 16:40-17:15
        </h1>

        <ul>
        <li>Wei Liu (Head of Machine Learning Research at Nuro) [<a href="https://www.youtube.com/watch?v=ezRe6sL3-Rw">video</a>]</li>
        </ul>

       <b>Title:</b>  Achievements and Open Questions for Autonomous Driving
       <br>
        <b>Abstract:</b> Autonomous Driving is both an exciting and challenging problem. In this talk, I will discuss the major achievements of using machine learning to transform how we think and tackle problems in perception, prediction, and planning at Nuro in the last 6 years. Despite all the achievements, there are still many open questions that we need to solve before we can really scale up autonomous driving to better everyday life. In particular, I will outline a high-level idea of how we can do joint prediction and planning with model-based reinforcement learning and call out the key components we need to build for it and the challenges.

        <h1>
        <b>Lightning Talks II</b>: 17:15-17:55
        </h1>

        <ol>
        <li><em>"Human-vehicle Cooperative Visual Perception for Autonomous Driving under Complex Traffic Environments"</em>, Yiyue Zhao, Cailin Lei, Yu Shen, Yuchuan Du and Qijun Chen (<a href="https://drive.google.com/file/d/1ms93WUZbomEZSMYCChoaywm1gxNAxku8/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=oFKvb2FuvTQ">video</a>)</li>
        <li><em>"MCIP: Multi-Stream Network for Pedestrian Crossing Intention Prediction"</em>, Je-Seok Ham, Kangmin Bae and Jinyoung Moon (<a href="https://drive.google.com/file/d/1Lj8BCCu4fVuqVmAX2u39EnHSqqNJZuVW/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=aX0c-LzWdVg">video</a>)</li>
        <li><em>"SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking"</em>, Ziqi Pang, Zhichao Li and Naiyan Wang (<a href="https://drive.google.com/file/d/1mQk_96FLWn457VCi78oRfaBbPQKDxjsV/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=pjdAPCny8oA">video</a>)</li>
        <li><em>"Ego-Motion Compensation of Range-Beam-Doppler Radar Data for Object Detection"</em>, Michael Meyer, Marc Unzueta, Georg Kuschk and Sven Tomforde (<a href="https://drive.google.com/file/d/1JgUbKLwUgDuDnNm9Xfez8QSwWYrRmiPW/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=BYltUsKvwBk">video</a>)</li>
        <li><em>"RPR-Net: A Point Cloud-based Rotation-aware Large Scale Place Recognition Network"</em>, Zhaoxin Fan, Zhenbo Song, Wenping Zhang, Hongyan Liu, Jun He and Xiaoyong Du (<a href="https://drive.google.com/file/d/1v_opP-ub6fAipIP-g-BNAYIEge76p7O1/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=VlljuKslA9Q">video</a>)</li>
        <li><em>"Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network"</em>, Yuhang He, Lin Chen, Junkun Xie and Long Chen (<a href="https://drive.google.com/file/d/1q69vjuJXITe68Yy8smaDPb213Y0E1OU0/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=SDHd3pru3pc">video</a>)</li>
        <li><em>"SIM2E: Benchmarking the Group Equivariant Capability of Correspondence Matching Algorithms"</em>, Shuai Su, Zhongkai Zhao, Yixin Fei, Shuda Li, Qijun Chen and Rui Fan (<a href="https://drive.google.com/file/d/1aRjmKsN84_q8-IZbGj3-wUTHwvuSWnsC/view?usp=sharing">full paper</a>, <a href="https://www.youtube.com/watch?v=BBTV2mue-40">video</a>)</li>
        <li><em>"Number-Adaptive Prototype Learning for 3D Point Cloud Semantic Segmentation"</em>, Yangheng Zhao, Jun Wang, Xiaolong Li, Yue Hu, Ce Zhang, Siheng Chen and Yanfeng Wang (<a href="https://drive.google.com/file/d/1Cnue_LRA6VpfLEUtH-cxq84qQc5M6KWO/view?usp=sharing">extended abstract</a>, <a href="https://www.youtube.com/watch?v=bP47wrTJ7Vs">video</a>)</li>
        <li><em>"Effectiveness of Function Matching in Driving Scene Recognition"</em>, Shingo Yashima (<a href="https://drive.google.com/file/d/1-GfrvbFg4DNfU24-aaclFthez7tw-Zb2/view?usp=sharing">extended abstract</a>, <a href="https://www.youtube.com/watch?v=HGXGERdSjtE">video</a>)</li>
        </ol>

        <h1>
        <b>Closing Remarks</b>: 17:55-18:00
        </h1>

    </div>
</section>
    
    
<section class="main-section paddind" id="sponsor">
    <div class="container " >
        <h2>Sponsors</h2>
<br>
        <div class="portfolioContainer_new wow fadeInUp delay-02s text-align:center" >

            <div class="Portfolio-box printdesign" >
                <a href=""><img src="img/nvidia-logo.png" alt=""></a>
            </div>


        </div>
    </div>

</section>

<br>
<br>
<br>

<div class="container" style="margin-top: 5rem">
    <h2>Contact</h2>
    <section class="main-section contact" id="contact">

        <div class="row" >
            <div class="col-lg-6 col-sm-7 wow fadeInLeft">


                <div class="contact-info-box phone clearfix">
                    <h3><i class="fa-phone"></i>Phone: +1 (412) 710-6868</h3>
                    
                </div>
                <div class="contact-info-box email clearfix">
                    <h3><i class="fa-envelope"></i>E-Mail: avvision@mias.group</h3>
                    
                </div>
                <ul class="social-link">
                    <li class="twitter"><a href="https://twitter.com/mias_avvision"><i class="fa-twitter"></i></a></li>
                    <li class="facebook"><a href="https://www.facebook.com/AVVision-102524081821979/"><i class="fa-facebook"></i></a></li>
                    <li class="pinterest"><a href="https://www.youtube.com/channel/UCL0Uxgn1f_3Dup31dtWe-QQ"><i class="fa-youtube"></i></a></li>
                    <li class="gplus"><a href="img/qr_code.jpg"><i class="fa-wechat"></i></a></li>
                </ul>
            </div>
            <div class="col-lg-6 col-sm-5 wow fadeInUp delay-05s">
                
                <div class="form">

                    <div id="sendmessage">Your message has been sent. Thank you!</div>
                    <div id="errormessage"></div>
                    <form action="mailto:avvision@mias.group" method="post" enctype="text/plain">
                        <div class="form-group">
                            <input type="text" name="Name: " class="form-control form-footer" placeholder="Name" required="Name">
                            <div class="validation"></div>
                        </div>
                        <div class="form-group">
                            <input type="text" name="Email: " class="form-control form-footer" placeholder="Email" required="Email">
                            <div class="validation"></div>
                        </div>
                        <div class="form-group">
                            <textarea type="text" name="Message: " class="form-control form-footer" rows="10" placeholder="Message" required="Message"></textarea>
                            <div class="validation"></div>
                        </div>

                        <div class="text-center"><button type="submit" class="input-btn">Send Message</button></div>
                    </form>
                </div>



            </div>
        </div>
    </section>
</div>


<!-- Messenger Chat plugin Code -->
<div id="fb-root"></div>

<!-- Your Chat plugin code -->
<div id="fb-customer-chat" class="fb-customerchat">
</div>

<script>
  var chatbox = document.getElementById('fb-customer-chat');
  chatbox.setAttribute("page_id", "102524081821979");
  chatbox.setAttribute("attribution", "biz_inbox");

  window.fbAsyncInit = function() {
    FB.init({
      xfbml            : true,
      version          : 'v11.0'
    });
  };

  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = 'https://connect.facebook.net/en_US/sdk/xfbml.customerchat.js';
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<footer class="footer">
    <div class="container">
        <div class="footer-logo"><a href="https://avvision.xyz/"><img src="img/logo.png" alt=""></a></div>
        <span class="copyright">&copy; 2022 AVVision. All Rights Reserved.</span>

    </div>
</footer>


<script type="text/javascript">
    $(document).ready(function(e) {
        $('#test').scrollToFixed();
        $('.res-nav_click').click(function(){
            $('.main-nav').slideToggle();
            return false

        });

    });
</script>

<script>
    wow = new WOW(
        {
            animateClass: 'animated',
            offset:       100
        }
    );
    wow.init();
</script>


<script type="text/javascript">
    $(window).load(function(){

        $('.main-nav li a, .servicelink').bind('click',function(event){
            var $anchor = $(this);

            $('html, body').stop().animate({
                scrollTop: $($anchor.attr('href')).offset().top - 102
            }, 1500,'easeInOutExpo');


            if ($(window).width() < 768 ) {
                $('.main-nav').hide();
            }
            event.preventDefault();
        });
    })
</script>


<script type="text/javascript">
    (function(){
        var designW = 1080;  //设计稿宽
        var font_rate = 10;
        //适配
        document.getElementsByTagName("html")[0].style.fontSize = document.body.offsetWidth / designW * font_rate + "px";
        document.getElementsByTagName("body")[0].style.fontSize = document.body.offsetWidth / designW * font_rate + "px";

        //监测窗口大小变化
        window.addEventListener("onorientationchange" in window ? "orientationchange" : "resize", function() {
            document.getElementsByTagName("html")[0].style.fontSize = document.body.offsetWidth / designW * font_rate + "px";
            document.getElementsByTagName("body")[0].style.fontSize = document.body.offsetWidth / designW * font_rate + "px";
        }, false);

    })();
</script>

<script type="text/javascript">

    $(window).load(function(){


        var $container = $('.portfolioContainer'),
            $body = $('body'),
            colW = 375,
            columns = null;

        var a ;
        var speakers;
        a= document.getElementById('my_speakers');
        speakers = a.getElementsByTagName('div').length;

        $container.isotope({
            // disable window resizing
            resizable: true,
            masonry: {
                columnWidth: colW
            }
        });

        $(window).smartresize(function(){
            // check if columns has changed
            var currentColumns = Math.floor( ( $body.width() -30 ) / colW );

            if ( currentColumns !== columns ) {
                // set new column count
                columns = currentColumns;
                // apply width to container manually, then trigger relayout
                $container.width( columns * colW )
                    .isotope('reLayout');
            }

        }).smartresize(); // trigger resize to set container width
        $('.portfolioFilter a').click(function(){
            $('.portfolioFilter .current').removeClass('current');
            $(this).addClass('current');

            var selector = $(this).attr('data-filter');
            $container.isotope({

                filter: selector,
            });
            return false;
        });

    });

</script>

</body>
</html>
